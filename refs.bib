
@article{gebru_datasheets_2021,
	title = {Datasheets for datasets},
	volume = {64},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/3458723},
	doi = {10.1145/3458723},
	abstract = {Documentation to facilitate communication between dataset creators and consumers.},
	number = {12},
	urldate = {2024-05-18},
	journal = {Communications of the ACM},
	author = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and III, Hal Daumé and Crawford, Kate},
	month = nov,
	year = {2021},
	pages = {86--92},
}

@article{moliner_two-stage_2022,
	title = {A {Two}-{Stage} {U}-{Net} for {High}-{Fidelity} {Denoising} of {Historical} {Recordings}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2202.08702},
	doi = {10.48550/ARXIV.2202.08702},
	abstract = {Enhancing the sound quality of historical music recordings is a long-standing problem. This paper presents a novel denoising method based on a fully-convolutional deep neural network. A two-stage U-Net model architecture is designed to model and suppress the degradations with high fidelity. The method processes the time-frequency representation of audio, and is trained using realistic noisy data to jointly remove hiss, clicks, thumps, and other common additive disturbances from old analog discs. The proposed model outperforms previous methods in both objective and subjective metrics. The results of a formal blind listening test show that real gramophone recordings denoised with this method have significantly better quality than the baseline methods. This study shows the importance of realistic training data and the power of deep learning in audio restoration.},
	urldate = {2024-05-18},
	author = {Moliner, Eloi and Välimäki, Vesa},
	year = {2022},
	keywords = {Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
}

@inproceedings{inproceedings,
author = {Veaux, Christophe and Yamagishi, Junichi and King, Simon},
year = {2013},
month = {11},
pages = {1-4},
title = {The voice bank corpus: Design, collection and data analysis of a large regional accent speech database},
doi = {10.1109/ICSDA.2013.6709856}
}

@inproceedings{Pirker2011APT,
  title={A Pitch Tracking Corpus with Evaluation on Multipitch Tracking Scenario},
  author={Gregor Pirker and Michael Wohlmayr and {\vS}tefan Petr{\'i}k and Franz Pernkopf},
  booktitle={Interspeech},
  year={2011},
  url={https://api.semanticscholar.org/CorpusID:13012536}
}

@misc{thiemann_demand_2013,
	title = {Demand: {A} {Collection} {Of} {Multi}-{Channel} {Recordings} {Of} {Acoustic} {Noise} {In} {Diverse} {Environments}},
	copyright = {Creative Commons Attribution 4.0, Open Access},
	shorttitle = {Demand},
	url = {https://zenodo.org/record/1227120},
	doi = {10.5281/ZENODO.1227120},
	abstract = {{\textless}strong{\textgreater}DEMAND: Diverse Environments Multichannel Acoustic Noise Database{\textless}/strong{\textgreater}

A database of 16-channel environmental noise recordings

{\textless}strong{\textgreater}Introduction{\textless}/strong{\textgreater}

Microphone arrays, a (typically regular) arrangement of several microphones, allow for a number of interesting signal processing techniques. The correlation of audio signals from microphones that are located in close proximity with each other can, for example, be used to determine the spatial location of sound source relative to the array, or to isolate or enhance a signal based on the direction from which the sound reaches the array.

Typically, experiments with microphone arrays that consider acoustic background noise use controlled environments or simulated environments. Such artificial setups will in general be sparse in terms of noise sources. Other pre-existing real-world noise databases (e.g. the AURORA-2 corpus, the CHiME background noise data, or the NOISEX-92 database) tend to provide only a very limited variety of environments and are limited to at most 2 channels.

The DEMAND (Diverse Environments Multichannel Acoustic Noise Database) presented here provides a set of recordings that allow testing of algorithms using real-world noise in a variety of settings. This version provides 15 recordings. All recordings are made with a 16-channel array, with the smallest distance between microphones being 5 cm and the largest being 21.8 cm.

{\textless}strong{\textgreater}License{\textless}/strong{\textgreater}

This work, the audio data and the document describing it, is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License.

{\textless}strong{\textgreater}The data{\textless}/strong{\textgreater}

A description of the data and the recording equipment is provided in the file {\textless}strong{\textgreater}DEMAND.pdf{\textless}/strong{\textgreater}. All recordings are available as 16 single-channel WAV files in one directory at both 48 kHz and 16 kHz sampling rates. All files are compressed into "zip" files.

{\textless}strong{\textgreater}Other information{\textless}/strong{\textgreater}

The MATLAB scripts listed in the documentation can be found in the file {\textless}strong{\textgreater}scripts.zip{\textless}/strong{\textgreater}.

{\textless}strong{\textgreater}The Authors{\textless}/strong{\textgreater}

This work was created by Joachim Thiemann (IRISA-CNRS), Nobutaka Ito (University of Tokyo), and Emmanuel Vincent (Inria Rennes - Bretagne Atlantique). It was supported by Inria under the Associate Team Program VERSAMUS.},
	language = {en},
	urldate = {2024-05-18},
	publisher = {[object Object]},
	author = {Thiemann, Joachim and Ito, Nobutaka and Vincent, Emmanuel},
	month = jun,
	year = {2013},
	keywords = {Microphone Array, Multichannel Audio, Noise},
}

@article{reddy2019scalable,
  title={A Scalable Noisy Speech Dataset and Online Subjective Test Framework},
  author={Reddy, Chandan KA and Beyrami, Ebrahim and Pool, Jamie and Cutler, Ross and Srinivasan, Sriram and Gehrke, Johannes},
  journal={Proc. Interspeech 2019},
  pages={1816--1820},
  year={2019}
}

@book{verhaegh_algorithms_2004,
	address = {Dordrecht},
	series = {Philips {Research}},
	title = {Algorithms in {Ambient} {Intelligence}},
	volume = {2},
	copyright = {http://www.springer.com/tdm},
	isbn = {9789048164905 9789401707039},
	url = {http://link.springer.com/10.1007/978-94-017-0703-9},
	language = {en},
	urldate = {2024-05-18},
	publisher = {Springer Netherlands},
	editor = {Verhaegh, Wim F. J. and Aarts, Emile and Korst, Jan and Toolenaar, Frank},
	year = {2004},
	doi = {10.1007/978-94-017-0703-9},
}


@article{mcloughlin_timefrequency_2020,
	title = {Time–{Frequency} {Feature} {Fusion} for {Noise} {Robust} {Audio} {Event} {Classification}},
	volume = {39},
	issn = {0278-081X, 1531-5878},
	url = {http://link.springer.com/10.1007/s00034-019-01203-0},
	doi = {10.1007/s00034-019-01203-0},
	language = {en},
	number = {3},
	urldate = {2024-05-18},
	journal = {Circuits, Systems, and Signal Processing},
	author = {McLoughlin, Ian and Xie, Zhipeng and Song, Yan and Phan, Huy and Palaniappan, Ramaswamy},
	month = mar,
	year = {2020},
	pages = {1672--1687},
}

@article{arumugam_feature_2018,
	title = {Feature selection based on {MBFOA} for audio signal classification under consideration of {Gaussian} white noise},
	volume = {12},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	issn = {1751-9675, 1751-9683},
	url = {https://onlinelibrary.wiley.com/doi/10.1049/iet-spr.2016.0607},
	doi = {10.1049/iet-spr.2016.0607},
	language = {en},
	number = {6},
	urldate = {2024-05-18},
	journal = {IET Signal Processing},
	author = {Arumugam, Muthumari and Kaliappan, Mala},
	month = aug,
	year = {2018},
	pages = {777--785},
}

@article{varkonyi_dynamic_2023,
	title = {Dynamic noise filtering for multi-class classification of beehive audio data},
	volume = {213},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417422018681},
	doi = {10.1016/j.eswa.2022.118850},
	language = {en},
	urldate = {2024-05-18},
	journal = {Expert Systems with Applications},
	author = {Várkonyi, Dániel Tamás and Seixas, José Luis and Horváth, Tomáš},
	month = mar,
	year = {2023},
	pages = {118850},
}
